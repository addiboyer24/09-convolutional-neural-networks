{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 9 - Convolutional Neural Networks\n",
    "\n",
    "## Reading (or watching, as the case may be): \n",
    "(Andrew Ng Stanford Lecture)[https://www.youtube.com/watch?v=bNb2fEVKeEo]\n",
    "\n",
    "## IC9A Implement Convolution\n",
    "Implement a function that takes an arbitrary 1D input and an arbitrary 1D kernel, and outputs their convolution.  An easy way to check if your implementation is correct is to apply the kernel \n",
    "$$\n",
    "[-1,0,1]\n",
    "$$\n",
    "to a sine function, and see if it returns (a scaled version of) cosine.  Apply a moving average kernel to the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, 3, 5, 3, 1] convolved with [0.3333333333333333, 0.3333333333333333, 0.3333333333333333] = [3.0, 3.6666666666666665, 3.0]\n",
      "Max pooling of [1, 3, 5, 7, 9, 11] with stride = 3 = [5, 11]\n"
     ]
    }
   ],
   "source": [
    "x = [1,3,5,3,1]\n",
    "kernel = [1/3,1/3,1/3]\n",
    "\n",
    "def 1d_convolution(x, kernel, stride=1):\n",
    "    h = []\n",
    "    for i in range(0, len(x) - len(kernel)+1, stride):\n",
    "        tot = 0\n",
    "        for j in range(len(kernel)):\n",
    "            tot += kernel[j]* x[i+j]\n",
    "        h.append(tot)\n",
    "    \n",
    "    return h\n",
    "\n",
    "answer = 1d_convolution(x, kernel)\n",
    "print('x: {} convolved with {} = {}'.format(x, kernel, answer))\n",
    "\n",
    "\n",
    "x = [1,3,5,7,9,11]\n",
    "stride = 3\n",
    "def max_pooling(x, stride=1):\n",
    "    h = []\n",
    "    for i in range(0,len(x), stride):\n",
    "        h.append(max(x[i:i+stride]))\n",
    "    return h\n",
    "\n",
    "answer = max_pooling(x, stride)\n",
    "print('Max pooling of {} with stride = {} = {}'.format(x, stride, answer))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Implementing a CNN for song classification\n",
    "\n",
    "First of all, we need a dataset.  I've grabbed one from the internet that consists of 100 songs per genre.  We'll limit ourselves to blues, metal, country, and hiphop.  I can use scipy to import .wav files and create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "genres = ['blues','metal','country','hiphop']\n",
    "\n",
    "N=len(genres)\n",
    "\n",
    "wavlist = []\n",
    "labels = []\n",
    "\n",
    "# blues=0,metal=1,country=2,hiphop=3\n",
    "for i,genre in enumerate(genres):\n",
    "    files = os.listdir('data/genres/'+genre)\n",
    "    for f in files:\n",
    "        filename = 'data/genres/'+genre+'/'+f\n",
    "        count,data = wavfile.read(filename)\n",
    "        \n",
    "        # Here, I am downsampling the data by a factor of 8, and keeping only the first 65536 features,\n",
    "        # roughly one minute of song\n",
    "        wavlist.append(data[:2**19:8]/2**15)\n",
    "        labels.append(i)\n",
    "       \n",
    "y = np.array(labels)\n",
    "X = np.vstack(wavlist)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot some examples of the wave-forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,axs = plt.subplots(nrows=4,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "for ax,index in zip(axs,[0,100,200,300]):\n",
    "    ax.plot(X[index,:])\n",
    "    ax.set_xlim(0,10000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are definitely differences, but maybe that's just random.  What does a couple examples of metal look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,axs = plt.subplots(nrows=4,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "for ax,index in zip(axs,[300,301,302,303]):\n",
    "    ax.plot(X[index,:])\n",
    "    ax.set_xlim(0,10000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't know.  We'll see if we can extract some characteristic features.  To do this, we'll learn filters which are then *convolved* over the signal.  \n",
    "\n",
    "Despite having relatively few examples (100 for each genre!), we'll want to ensure that our method can generalize, so let's split our data into test and training sets.  At the same time, we'll want to reshape our arrays into what pytorch expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training/test set split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "X_train = X_train.reshape((*X_train.shape,1))\n",
    "X_test = X_test.reshape((*X_test.shape,1))\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do our normal ritual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "X_train = X_train.to(torch.float32)\n",
    "\n",
    "#Pytorch expects channels first, so reshape\n",
    "X_train = X_train.reshape(-1,1,2**16)\n",
    "X_test = X_test.to(torch.float32)\n",
    "X_test = X_test.reshape(-1,1,2**16)\n",
    "y_train = y_train.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 64\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters and functions.\n",
    "        Here, we instantiate four 1D convolution layers and four max pooling layers\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        # Conv1d arguments: number of input feature maps, number of output feature maps, kernel width, edge padding\n",
    "        self.conv1 = nn.Conv1d(1,10,9,padding=4)\n",
    "        # MaxPool1d arguments: kernel width, edge padding\n",
    "        self.pool1 = nn.MaxPool1d(16,padding=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(10,10,9,padding=4)\n",
    "        self.pool2 = nn.MaxPool1d(16,padding=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(10,10,9,padding=4)\n",
    "        self.pool3 = nn.MaxPool1d(16,padding=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(10,10,9,padding=4)\n",
    "        self.pool4 = nn.MaxPool1d(16,padding=4)\n",
    "        \n",
    "        # Final linear transformation layer (10 features get output from final convolution, mapped to four\n",
    "        # Softmax inputs)\n",
    "        self.l1 = nn.Linear(10,4)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  \n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply convolution\n",
    "        a1 = self.conv1(x)\n",
    "        \n",
    "        # Apply activation function\n",
    "        z1 = torch.relu(a1)\n",
    "        \n",
    "        # Apply max pooling\n",
    "        z1 = self.pool1(z1)\n",
    "                \n",
    "        a2 = self.conv2(z1)\n",
    "        z2 = torch.relu(a2)\n",
    "        z2 = self.pool2(z2)\n",
    "        \n",
    "        a3 = self.conv3(z2)\n",
    "        z3 = torch.relu(a3)\n",
    "        z3 = self.pool3(z3)\n",
    "        \n",
    "        a4 = self.conv4(z3)\n",
    "        z4 = torch.relu(a4)\n",
    "        z4 = self.pool4(z4)        \n",
    "        \n",
    "        # Flatten the array (basically removes singleton dimension)\n",
    "        z_flat = torch.reshape(z4,(-1,10))\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        z_out = self.l1(z_flat)\n",
    "        \n",
    "        # Output logits to softmax\n",
    "        return z_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our training proceeds very much as it has in the past, because our neural network is simply a function from inputs to outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)   \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    if epoch%10==0:\n",
    "        total=0.\n",
    "        correct=0.\n",
    "        # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "        for d,t in test_loader:\n",
    "            outputs = model(d)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += float(t.size(0))\n",
    "            correct += float((predicted==t).sum())\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        for d,t in train_loader:\n",
    "            outputs = model(d)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total_train += float(t.size(0))\n",
    "            correct_train += float((predicted==t).sum())\n",
    "        \n",
    "        # Print the epoch, the training loss, and the test set accuracy.\n",
    "        train_accs.append(100.*correct_train/total_train)\n",
    "        test_accs.append(100.*correct/total)\n",
    "\n",
    "        print(epoch,loss.item(),train_accs[-1],test_accs[-1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accs,label='Training accuracy')\n",
    "plt.plot(test_accs,label='Test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, probably.  However, all of our old techniques for regularization can still apply here. \n",
    "\n",
    "## 2 Application to images.\n",
    "While CNNs are cool for 1D sequences, their primary application has been in the field of image analysis.  Next time, we'll see how these networks provide high-grade performance on some interesting and difficult image classification tasks!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
